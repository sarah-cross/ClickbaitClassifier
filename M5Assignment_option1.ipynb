{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['aaa', 'aaevpc', 'aaron', ..., 'zurich', 'zykina', 'z≈Çoty'],\n      dtype=object)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(21440, 18354)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in clickbait data, set target as 1 (positive for clickbait)\n",
    "clickbait = pd.read_csv('clickbait_data.gz', compression='gzip', header=None, delimiter='\\t', quotechar='\"', names=['message', 'target'])\n",
    "clickbait['target'] = 1\n",
    "\n",
    "# Read in non-clickbait data, set target as 0 (negative for clickbait)\n",
    "non_clickbait = pd.read_csv('non_clickbait_data.gz', compression='gzip', header=None, delimiter='\\t', quotechar='\"', names=['message', 'target'])\n",
    "non_clickbait['target'] = 0\n",
    "\n",
    "# Concatenate clickbait and non-clickbait into one dataframe\n",
    "data = pd.concat([clickbait, non_clickbait])\n",
    "\n",
    "# Set up X and y and perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['target'], test_size=0.33)\n",
    "\n",
    "# Create a count vectorizer (bag of words)\n",
    "# Fit and transform vectorizer to text\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', token_pattern='[^\\W\\d_]+')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Verify\n",
    "#display(vectorizer.get_feature_names_out())\n",
    "#display(X_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNN:  {'n_neighbors': 2}\n",
      "K-Nearest Neighbors Training Score:  0.9979011194029851\n",
      "K-Nearest Neighbors Testing Score:  0.8794507575757575\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn_param_grid = { 'n_neighbors': range(1, 4) }\n",
    "knn_model = GridSearchCV(knn, knn_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for KNN: \", knn_model.best_params_)\n",
    "print(\"K-Nearest Neighbors Training Score: \", knn_model.score(X_train, y_train))\n",
    "print(\"K-Nearest Neighbors Testing Score: \", knn_model.score(X_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Cross Validation Score:  0.8783582089552239\n",
      "K-Nearest Neighbors Cross Validation Standard Deviation:  0.015047020588175428\n"
     ]
    }
   ],
   "source": [
    "print(\"K-Nearest Neighbors Cross Validation Score: \", knn_model.cv_results_['mean_test_score'][1])\n",
    "print(\"K-Nearest Neighbors Cross Validation Standard Deviation: \", knn_model.cv_results_['std_test_score'][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Naive Bayes:  {'alpha': 0.5}\n",
      "Naive Bayes Training Score:  0.9819029850746268\n",
      "Naive Bayes Testing Score:  0.9585227272727272\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb_param_grid = { 'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1] }\n",
    "mnb_model = GridSearchCV(mnb, mnb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "mnb_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for Naive Bayes: \", mnb_model.best_params_)\n",
    "print(\"Naive Bayes Training Score: \", mnb_model.score(X_train, y_train))\n",
    "print(\"Naive Bayes Testing Score: \", mnb_model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Cross Validation Score:  0.9559235074626866\n",
      "Naive Bayes Cross Validation Standard Deviation:  0.001978843604066819\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Cross Validation Score: \", mnb_model.cv_results_['mean_test_score'][4])\n",
    "print(\"Naive Bayes Cross Validation Standard Deviation: \", mnb_model.cv_results_['std_test_score'][4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Multilayer Perceptron:  {'hidden_layer_sizes': [30, 15]}\n",
      "Multilayer Perceptron Training Score:  1.0\n",
      "Multilayer Perceptron Testing Score:  0.9543560606060606\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron\n",
    "mlp = MLPClassifier()\n",
    "mlp_param_grid = { 'hidden_layer_sizes': [[40, 20], [30, 15], [20, 10], [10, 5]]}\n",
    "mlp_model = GridSearchCV(mlp, mlp_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "print(\"Best parameters for Multilayer Perceptron: \", mlp_model.best_params_)\n",
    "print(\"Multilayer Perceptron Training Score: \", mlp_model.score(X_train, y_train))\n",
    "print(\"Multilayer Perceptron Testing Score: \", mlp_model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron Cross Validation Score:  0.950419776119403\n",
      "Multilayer Perceptron Cross Validation Standard Deviation:  0.0033066365437996307\n"
     ]
    }
   ],
   "source": [
    "print(\"Multilayer Perceptron Cross Validation Score: \", mlp_model.cv_results_['mean_test_score'][1])\n",
    "print(\"Multilayer Perceptron Cross Validation Standard Deviation: \", mlp_model.cv_results_['std_test_score'][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Report\n",
    "I used a Tfidf vectorizer to represent the data in order to adjust the weights of words that were used more/less frequently. I chose accuracy as a metric to show how often the prediction was equal to the label. Accuracy is particularly useful in this case because the data is evenly split. Also, because the data is evenly split, simply guessing clickbait every time would result in 50% accuracy. We expect each of the classifiers to do well above 50% in order to be useful.\n",
    "For the K-Nearest Neighbors classifier, cross validation chose 2 neighbors as its best parameters. K-Nearest Neighbor scored nearly perfect on training data, but there was a significant drop in performance on the testing data, which scored 87.95%. This is consistent with the cv model's score and standard deviation (87.83% +- 1.5% std dv). (The testing score of KNN improved by roughly 26% using a Tfidf vectorizer instead of a count vectorizer.) Because of the discrepancy between training and testing scores, K-Nearest Neighbors would probably not be the best choice in classifying clickbait.\n",
    "For the Multinomial Naive Bayes classifier, cross validation chose 0.5 as its best alpha parameter. Multinomial Naive Bayes scored well on both training and testing at 98.19% and 95.85% respectively. Testing scores are also very close to the cv model's score and standard deviation (95.59% +- 0.2% std dv).\n",
    "For the Multilayer Perceptron classifier, cross validation chose hidden layer size: [30, 15] as its best parameter. It scored perfectly on the training data and did well on the testing data with 95.44% accuracy. The test scores are very close to the cv model's score and standard deviation (95.04% +- 0.33%).\n",
    "The Multinomial Naive Bayes and Multilayer Perceptron had similar scores on the testing data. It's possible their performance could be even more improved by further editing the vocabulary. Both would be useful as a plugin for social media, web browsers, or news sites to prevent unwanted ads."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
